## Notes

- k-NN is a simple yet powerful algorithm for classification and regression tasks.

- One of the main advantages of k-NN is that it is a non-parametric algorithm, which means that it does not make any assumptions about the distribution of the data. This makes it very versatile and applicable to a wide range of problems.

- However, k-NN can be computationally expensive, especially when dealing with large datasets or high-dimensional data. In addition, the choice of the distance metric and the value of k can have a significant impact on the performance of the algorithm, and may need to be carefully tuned.

- The curse of dimensionality is an important concept to keep in mind when working with k-NN. As the number of dimensions in the data increases, the amount of data required to maintain a certain level of accuracy increases exponentially. This can make it difficult to apply k-NN to high-dimensional data.

- Scaling and normalization of the data can also be important when working with k-NN, as it can help to ensure that all features are given equal weight in the distance metric calculation.

- In terms of resources, there are many online tutorials, courses, and books available on k-NN and machine learning in general. It is also a widely studied topic in the academic literature, with many research papers exploring various aspects of the algorithm and its applications.